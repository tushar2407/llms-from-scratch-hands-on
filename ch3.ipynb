{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdf9a6e2",
   "metadata": {},
   "source": [
    "# Coding Attention Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eb82ea",
   "metadata": {},
   "source": [
    "## Attending to different parts of the input with self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32220b17",
   "metadata": {},
   "source": [
    "### simple self-attention without trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8af1c053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "494150ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5500, 0.8700, 0.6600])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_query = inputs[1] ## journey\n",
    "input_query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bb85b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4300, 0.1500, 0.8900])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1 = inputs[0]\n",
    "input_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96b97283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9544)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(input_query, input_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e8a166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(input_query, inputs.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32b8bf1",
   "metadata": {},
   "source": [
    "Above tensor, each element shows the similarity or attention score for the `input_query` with the corresponding `inputs`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6212a164",
   "metadata": {},
   "outputs": [],
   "source": [
    "## the below calculates the attention scores of second word wrt to each word in the sentence.\n",
    "attention_scores_2 = torch.softmax(torch.matmul(input_query, inputs.T), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85984fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d347ac1",
   "metadata": {},
   "source": [
    "Let's calculate the attention score for each word in the sentence wrt every other word in the sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec397c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
       "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
       "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
       "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
       "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
       "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(inputs, inputs.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0354b761",
   "metadata": {},
   "source": [
    "Every row is the attention score of the input word wrt every other word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33db62b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
       "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
       "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
       "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
       "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
       "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Now, let's normalize attention scores to not let the magnitudes introduces biases\n",
    "torch.softmax(torch.matmul(inputs, inputs.T), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd43bbec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4419, 0.6515, 0.5683])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector_2 = torch.matmul(attention_scores_2, inputs)\n",
    "context_vector_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b114700c",
   "metadata": {},
   "source": [
    "`attention_scores_2` corresponds with second element of the attention matrix as it should.\n",
    "\n",
    "`context_vector_2` is calculated as the weighted sum of the input vectors generally speaking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76bb2af",
   "metadata": {},
   "source": [
    "### simple self-attention without trainable weights for all tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e705916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
       "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
       "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
       "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
       "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
       "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.matmul(inputs, inputs.T), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e64a4b0",
   "metadata": {},
   "source": [
    "Attention score of each token with respect to all other tokens.\n",
    "\n",
    "Each row corresponds to the attention scores of a single token at that index.\n",
    "\n",
    "```\n",
    "[[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452], # your\n",
    "[0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581], # journey\n",
    "[0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565], # starts\n",
    "[0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720], # with\n",
    "[0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295], # one\n",
    "[0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]] # step\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc461f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.matmul(inputs, inputs.T), dim=-1).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085fd6b0",
   "metadata": {},
   "source": [
    "The above result shows that the normalization works and the sum of each attention vector is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f466a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421, 0.5931, 0.5790],\n",
       "        [0.4419, 0.6515, 0.5683],\n",
       "        [0.4431, 0.6496, 0.5671],\n",
       "        [0.4304, 0.6298, 0.5510],\n",
       "        [0.4671, 0.5910, 0.5266],\n",
       "        [0.4177, 0.6503, 0.5645]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores = inputs @ inputs.T\n",
    "attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "context_vector = attention_weights @ inputs\n",
    "context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c501b039",
   "metadata": {},
   "source": [
    "The above matrix, each row is the context vector for the corresponding input tokens. \n",
    "\n",
    "```\n",
    "[[0.4421, 0.5931, 0.5790], # Your\n",
    "[0.4419, 0.6515, 0.5683], # Journey \n",
    "[0.4431, 0.6496, 0.5671], # Starts\n",
    "[0.4304, 0.6298, 0.5510], # with \n",
    "[0.4671, 0.5910, 0.5266], # one\n",
    "[0.4177, 0.6503, 0.5645]] # step\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd95adb",
   "metadata": {},
   "source": [
    "### self-attention with trainable weights\n",
    "Now we will see that the inputs are not used as is, they are multiplied with the trainable weights which yield Q, K and V matrices hence the context vector is not the same dimensionality as the input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "27c6c840",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "22457515",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.randn(d_in,d_out))\n",
    "W_key = torch.nn.Parameter(torch.randn(d_in,d_out))\n",
    "W_value = torch.nn.Parameter(torch.randn(d_in,d_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a93557b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1115,  0.1204],\n",
       "        [-0.3696, -0.2404],\n",
       "        [-1.1969,  0.2093]], requires_grad=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "84b8ddd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = x_2 @ W_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "62814116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1729, -0.0048], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337beb6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1823, -0.6888],\n",
       "        [-0.1142, -0.7676],\n",
       "        [-0.1443, -0.7728],\n",
       "        [ 0.0434, -0.3580],\n",
       "        [-0.6467, -0.6476],\n",
       "        [ 0.3262, -0.3395]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d048b160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1196, -0.3566],\n",
       "        [ 0.4107,  0.6274],\n",
       "        [ 0.4091,  0.6390],\n",
       "        [ 0.2436,  0.4182],\n",
       "        [ 0.2653,  0.6668],\n",
       "        [ 0.2728,  0.3242]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = inputs @ W_value\n",
    "values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e8302b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1686,  0.2019],\n",
       "        [-1.1729, -0.0048],\n",
       "        [-1.1438, -0.0018],\n",
       "        [-0.6339, -0.0439],\n",
       "        [-0.2979,  0.0535],\n",
       "        [-0.9596, -0.0712]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = inputs @ W_query\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "528a2b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.1729, -0.0048], grad_fn=<SelectBackward0>),\n",
       " tensor([-0.1142, -0.7676], grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2 = query[1]\n",
    "keys_2 = keys[1]\n",
    "query_2, keys_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e485d938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1376, grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores_22 = query_2 @ keys_2\n",
    "attention_scores_22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "de41e6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2172,  0.1376,  0.1730, -0.0491,  0.7616, -0.3809],\n",
       "       grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores_2 = query_2 @ keys.T\n",
    "attention_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "00d6f2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1704, 0.1611, 0.1652, 0.1412, 0.2505, 0.1117],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k = keys.shape[1]\n",
    "torch.softmax(attention_scores_2 / d_k**0.5, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "96ee81da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(attention_scores_2 / d_k**0.5, dim=-1).sum(dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "775550eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2845, 0.4071],\n",
       "        [0.2854, 0.4081],\n",
       "        [0.2854, 0.4075],\n",
       "        [0.2864, 0.3974],\n",
       "        [0.2863, 0.3910],\n",
       "        [0.2860, 0.4039]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores = query @ keys.T / d_k**0.5\n",
    "attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "context_vector = attention_weights @ values\n",
    "context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088bcbce",
   "metadata": {},
   "source": [
    "If you see, each element of the above matrix, again corresponds to each word, but the dimensions are different as we did not use the vector representation of the words directly rather their projections on a different plane where their dimensions shrunk (in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58ca027",
   "metadata": {},
   "source": [
    "### Implementing the self-attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3ec3c21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, bias = False):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias=bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias=bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        d_k = K.shape[-1]\n",
    "        \n",
    "        attentions_scores = Q @ K.T\n",
    "        attention_weights = torch.softmax(attentions_scores / d_k**0.5, dim=-1)\n",
    "        context_vector = attention_weights @ V\n",
    "        return context_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "eb542b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2410,  0.2378],\n",
       "        [-0.2419,  0.2406],\n",
       "        [-0.2418,  0.2403],\n",
       "        [-0.2409,  0.2409],\n",
       "        [-0.2392,  0.2355],\n",
       "        [-0.2420,  0.2433]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_v1 = SelfAttention(d_in, d_out)\n",
    "sa_v1(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc126e2f",
   "metadata": {},
   "source": [
    "## Hiding future words with causal attention\n",
    "### Applying causal attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f1405dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your journey starts with one step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "11003bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = sa_v1.W_q(inputs)\n",
    "K = sa_v1.W_k(inputs)\n",
    "V = sa_v1.W_v(inputs)\n",
    "d_k = K.shape[-1]\n",
    "\n",
    "attentions_scores = Q @ K.T\n",
    "attention_weights = torch.softmax(attentions_scores / d_k**0.5, dim=-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1fe34226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1566, 0.1692, 0.1691, 0.1690, 0.1662, 0.1699],\n",
       "        [0.1623, 0.1748, 0.1746, 0.1614, 0.1624, 0.1644],\n",
       "        [0.1619, 0.1743, 0.1741, 0.1621, 0.1628, 0.1649],\n",
       "        [0.1667, 0.1729, 0.1728, 0.1611, 0.1631, 0.1634],\n",
       "        [0.1560, 0.1622, 0.1623, 0.1756, 0.1702, 0.1738],\n",
       "        [0.1703, 0.1784, 0.1781, 0.1547, 0.1595, 0.1589]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "56f79bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = attentions_scores.shape[0]\n",
    "mask = torch.tril(torch.ones(context_length,context_length))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "64895627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1566, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1623, 0.1748, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1619, 0.1743, 0.1741, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1729, 0.1728, 0.1611, 0.0000, 0.0000],\n",
       "        [0.1560, 0.1622, 0.1623, 0.1756, 0.1702, 0.0000],\n",
       "        [0.1703, 0.1784, 0.1781, 0.1547, 0.1595, 0.1589]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_simple = attention_weights * mask\n",
    "masked_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54282447",
   "metadata": {},
   "source": [
    "But now the rows are not normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4d83036f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4814, 0.5186, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3173, 0.3416, 0.3412, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2475, 0.2567, 0.2565, 0.2392, 0.0000, 0.0000],\n",
       "        [0.1887, 0.1964, 0.1965, 0.2125, 0.2059, 0.0000],\n",
       "        [0.1703, 0.1784, 0.1781, 0.1547, 0.1595, 0.1589]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_simple/masked_simple.sum(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793f1f43",
   "metadata": {},
   "source": [
    "Above we first calculated `attn scores`(unnormalized) -> `attn weights`(normalized) -> `masked_values`(unnormalized) -> `attn values`(normalized)\n",
    "\n",
    "We can instead skip attention weights and do in 2 steps.\n",
    "`attn scores`-> `masked`  -> `attn values`(normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "63b4e90e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0523,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.1536,  0.0973,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.1483,  0.0933,  0.1177,    -inf,    -inf,    -inf],\n",
       "        [ 0.1031,  0.0750,  0.0887, -0.0083,    -inf,    -inf],\n",
       "        [ 0.0124, -0.0050,  0.0012, -0.0227,  0.1117,    -inf],\n",
       "        [ 0.1584,  0.1161,  0.1368, -0.0114,  0.4714, -0.2042]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length,context_length), diagonal=1)\n",
    "masked_simple = attention_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "masked_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7763bd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_weights = torch.softmax(masked_simple/keys.shape[0]**0.5, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8b5653e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5057, 0.4943, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3372, 0.3297, 0.3330, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2539, 0.2510, 0.2524, 0.2426, 0.0000, 0.0000],\n",
       "        [0.1994, 0.1980, 0.1985, 0.1965, 0.2076, 0.0000],\n",
       "        [0.1693, 0.1664, 0.1678, 0.1580, 0.1924, 0.1460]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d8081f",
   "metadata": {},
   "source": [
    "### Masking addtional attention weights with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8f0ffb8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dropout(p=0.5, inplace=False)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "layer = torch.nn.Dropout(p=0.5)\n",
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "df23d1bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 0., 2., 2., 0.],\n",
       "        [0., 0., 0., 2., 0., 2.],\n",
       "        [2., 2., 2., 2., 0., 2.],\n",
       "        [0., 2., 2., 0., 0., 2.],\n",
       "        [0., 2., 0., 2., 0., 2.],\n",
       "        [0., 2., 2., 2., 2., 0.]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(torch.ones(6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7a7fcde0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4285714285714286"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout = 0.3\n",
    "1 / (1 - dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d09347",
   "metadata": {},
   "source": [
    "### Causal self-attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1975c811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 3])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = torch.stack([inputs, inputs], dim=0)\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3ea9b3",
   "metadata": {},
   "source": [
    "So you can say the shape of the input batch is supposed to be `(batch_size, num_Tokens, input vector dimensions)`\n",
    "\n",
    "Below, `num_tokens` is to cut-off context length in case the batch contains a sequence of tokens that is longer than the context length, this does not happen in practice as that is taken care of before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72964a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, dropout, context_length, bias=False):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias=bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias=bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias=bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        Q = self.W_q(x) # (batch_size, num_tokens, d_out)\n",
    "        K = self.W_k(x) # (batch_size, num_tokens, d_out)\n",
    "        V = self.W_v(x) # (batch_size, num_tokens, d_out)\n",
    "        d_k = K.shape[-1]\n",
    "        \n",
    "        attention_scores = Q @ K.transpose(1,2)\n",
    "        attention_scores.masked_fill(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
    "            -torch.inf\n",
    "        )\n",
    "    \n",
    "        attention_weights = torch.softmax(attention_scores / d_k**0.5, dim=-1)\n",
    "        context_vector = attention_weights @ V\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c54795",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca = CausalAttention(d_in, d_out, 0.0, context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fedac8da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2410,  0.2378],\n",
       "         [-0.2419,  0.2406],\n",
       "         [-0.2418,  0.2403],\n",
       "         [-0.2409,  0.2409],\n",
       "         [-0.2392,  0.2355],\n",
       "         [-0.2420,  0.2433]],\n",
       "\n",
       "        [[-0.2410,  0.2378],\n",
       "         [-0.2419,  0.2406],\n",
       "         [-0.2418,  0.2403],\n",
       "         [-0.2409,  0.2409],\n",
       "         [-0.2392,  0.2355],\n",
       "         [-0.2420,  0.2433]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce44905",
   "metadata": {},
   "source": [
    "### Stacking Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ed72ef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_heads, dropout, context_length, bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, dropout, context_length, bias) for _ in range(num_heads)]\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        head_outputs = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        return head_outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5464508d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3948,  0.5029, -0.6581,  0.2660],\n",
       "         [-0.3927,  0.5003, -0.6597,  0.2673],\n",
       "         [-0.3929,  0.5005, -0.6594,  0.2670],\n",
       "         [-0.3953,  0.5035, -0.6584,  0.2633],\n",
       "         [-0.3979,  0.5066, -0.6540,  0.2605],\n",
       "         [-0.3933,  0.5011, -0.6610,  0.2659]],\n",
       "\n",
       "        [[-0.3948,  0.5029, -0.6581,  0.2660],\n",
       "         [-0.3927,  0.5003, -0.6597,  0.2673],\n",
       "         [-0.3929,  0.5005, -0.6594,  0.2670],\n",
       "         [-0.3953,  0.5035, -0.6584,  0.2633],\n",
       "         [-0.3979,  0.5066, -0.6540,  0.2605],\n",
       "         [-0.3933,  0.5011, -0.6610,  0.2659]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = batch.shape[1]\n",
    "d_in = batch.shape[-1]\n",
    "d_out = 2\n",
    "\n",
    "ma = MultiHeadAttention(d_in, d_out, 2, 0.0, context_length)\n",
    "ma(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbe8c12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
